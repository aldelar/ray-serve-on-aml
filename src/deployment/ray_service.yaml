# Make sure to increase resource requests and limits before using this example in production.
# For examples with more realistic resource configuration, see
# ray-cluster.complete.large.yaml and
# ray-cluster.autoscaler.large.yaml.
apiVersion: ray.io/v1alpha1
kind: RayService
metadata:
  name: many-models
spec:
  serviceUnhealthySecondThreshold: 300 # Config for the health check threshold for service. Default value is 60.
  deploymentUnhealthySecondThreshold: 300 # Config for the health check threshold for deployments. Default value is 60.
  serveConfig:
    importPath: ray_score_v2.deployment_graph
    runtimeEnv: |
      working_dir: "https://github.com/james-tn/Many-Models/raw/main/code/deployment/ray_many_models/script.zip"
      pip: ["azure-ai-ml"]
    deployments:
      - name: Deployment1
        numReplicas: 1
        userConfig: |
          tenant: tenant1
        rayActorOptions:
          numCpus: 0.2
      - name: Deployment2
        numReplicas: 1
        userConfig: |
          tenant: tenant2
      - name: Deployment3
        numReplicas: 1
        userConfig: |
          tenant: tenant3
        rayActorOptions:
          numCpus: 0.1
      - name: Deployment4
        numReplicas: 1
        userConfig: |
          tenant: tenant4
        rayActorOptions:
          numCpus: 0.1
      - name: Deployment5
        numReplicas: 1
        userConfig: |
          tenant: tenant5
        rayActorOptions:
          numCpus: 0.1
      - name: Deployment6
        numReplicas: 1
        userConfig: |
          tenant: tenant6
        rayActorOptions:
          numCpus: 0.1
      - name: Deployment7
        numReplicas: 1
        userConfig: |
          tenant: tenant7
        rayActorOptions:
          numCpus: 0.1
      - name: Deployment8
        numReplicas: 1
        userConfig: |
          tenant: tenant8
        rayActorOptions:
          numCpus: 0.1
      - name: Deployment9
        numReplicas: 1
        userConfig: |
          tenant: tenant9
        rayActorOptions:
          numCpus: 0.1
      - name: Deployment10
        numReplicas: 1
        userConfig: |
          tenant: tenant10
        rayActorOptions:
          numCpus: 0.1
      - name: Deploymentx
        numReplicas: 1
        userConfig: |
          tenant: default
        rayActorOptions:
          numCpus: 0.2
      - name: SharedMemory
        numReplicas: 1
        rayActorOptions:
          numCpus: 0.2
      - name: Dispatcher
        numReplicas: 2
        rayActorOptions:
          numCpus: 0.2
      - name: DAGDriver
        numReplicas: 2
        routePrefix: "/"
        rayActorOptions:
          numCpus: 0.1
  rayClusterConfig:
    rayVersion: '2.0.0' # should match the Ray version in the image of the containers
    ######################headGroupSpecs#################################
    # head group template and specs, (perhaps 'group' is not needed in the name)
    headGroupSpec:
      # Kubernetes Service Type, valid values are 'ClusterIP', 'NodePort' and 'LoadBalancer'
      serviceType: ClusterIP
      # the pod replicas in this group typed head (assuming there could be more than 1 in the future)
      replicas: 1
      # logical group name, for this called head-group, also can be functional
      # pod type head or worker
      # rayNodeType: head # Not needed since it is under the headgroup
      # the following params are used to complete the ray start: ray start --head --block --redis-port=6379 ...
      rayStartParams:
        port: '6379' # should match container port named gcs-server
        #include_webui: 'true'
        object-store-memory: '100000000'
        # webui_host: "10.1.2.60"
        dashboard-host: '0.0.0.0'
        num-cpus: '2' # can be auto-completed from the limits
        node-ip-address: $MY_POD_IP # auto-completed as the head pod IP
        block: 'true'
      #pod template
      template:
        metadata:
          labels:
            # custom labels. NOTE: do not define custom labels start with `raycluster.`, they may be used in controller.
            # Refer to https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/
            rayCluster: raycluster-sample # will be injected if missing
            rayNodeType: head # will be injected if missing, must be head or wroker
            groupName: headgroup # will be injected if missing
          # annotations for pod
          annotations:
            key: value
        spec:
          containers:
            - name: ray-head
              image: ws02acrrqjiouba.azurecr.io/custom-ray-serve:v1
              imagePullPolicy: Always
              #image: bonsaidev.azurecr.io/bonsai/lazer-0-9-0-cpu:dev
              env:
                - name: MY_POD_IP
                  valueFrom:
                    fieldRef:
                      fieldPath: status.podIP
                - name: REDIS_HOST
                  value: 
                - name: REDIS_KEY
                  value: 
              resources:
                limits:
                  cpu: 5
                  memory: 20Gi
                requests:
                  cpu: 3
                  memory: 10Gi
              ports:
                - containerPort: 6379
                  name: gcs-server
                - containerPort: 8265 # Ray dashboard
                  name: dashboard
                - containerPort: 10001
                  name: client
                - containerPort: 8000
                  name: serve
    workerGroupSpecs:
      # the pod replicas in this group typed worker
      - replicas: 1
        minReplicas: 1
        maxReplicas: 15
        # logical group name, for this called small-group, also can be functional
        groupName: small-group
        # if worker pods need to be added, we can simply increment the replicas
        # if worker pods need to be removed, we decrement the replicas, and populate the podsToDelete list
        # the operator will remove pods from the list until the number of replicas is satisfied
        # when a pod is confirmed to be deleted, its name will be removed from the list below
        #scaleStrategy:
        #  workersToDelete:
        #  - raycluster-complete-worker-small-group-bdtwh
        #  - raycluster-complete-worker-small-group-hv457
        #  - raycluster-complete-worker-small-group-k8tj7
        # the following params are used to complete the ray start: ray start --block --node-ip-address= ...
        rayStartParams:
          node-ip-address: $MY_POD_IP
          block: 'true'
        #pod template
        template:
          metadata:
            labels:
              key: value
            # annotations for pod
            annotations:
              key: value
          spec:
            initContainers:
              # the env var $RAY_IP is set by the operator if missing, with the value of the head service name
              - name: init-myservice
                image: busybox:1.28
                command: ['sh', '-c', "until nslookup $RAY_IP.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done"]
            containers:
              - name: machine-learning # must consist of lower case alphanumeric characters or '-', and must start and end with an alphanumeric character (e.g. 'my-name',  or '123-abc'
                image: ws02acrrqjiouba.azurecr.io/custom-ray-serve:v1
                imagePullPolicy: Always
                # environment variables to set in the container.Optional.
                # Refer to https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/
                env:
                  - name:  RAY_DISABLE_DOCKER_CPU_WARNING
                    value: "1"
                  - name: TYPE
                    value: "worker"
                  - name: CPU_REQUEST
                    valueFrom:
                      resourceFieldRef:
                        containerName: machine-learning
                        resource: requests.cpu
                  - name: CPU_LIMITS
                    valueFrom:
                      resourceFieldRef:
                        containerName: machine-learning
                        resource: limits.cpu
                  - name: MEMORY_LIMITS
                    valueFrom:
                      resourceFieldRef:
                        containerName: machine-learning
                        resource: limits.memory
                  - name: MEMORY_REQUESTS
                    valueFrom:
                      resourceFieldRef:
                        containerName: machine-learning
                        resource: requests.memory
                  - name: MY_POD_NAME
                    valueFrom:
                      fieldRef:
                        fieldPath: metadata.name
                  - name: MY_POD_IP
                    valueFrom:
                      fieldRef:
                        fieldPath: status.podIP
                ports:
                  - containerPort: 80
                    name: client
                lifecycle:
                  preStop:
                    exec:
                      command: ["/bin/sh","-c","ray stop"]
                resources:
                  limits:
                    cpu: "5"
                    memory: "20Gi"
                  requests:
                    cpu: "3"
                    memory: "10Gi"

